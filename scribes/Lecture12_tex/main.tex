\documentclass{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}
\usepackage{amssymb}
\usepackage{amsmath,amsfonts,graphicx}
\usepackage{hyperref}
\usepackage{enumitem}

\newcounter{lecnum}

\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\DeclareMathOperator*{\E}{\mathbb{E}}
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CS 768: Learning With Graphs
        \hfill Autumn 2020-2021} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Instructor: Prof. Abir De \hfill Scribe: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
            \vspace{#2}
            \begin{center}
            Figure \thelecnum.#1:~#3
            \end{center}
    }
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:
\newcommand{\Sim}{\text{sim}}

\begin{document}

\lecture{12}{Deep learning over Graphs}{}{Anshul Tomar \& Anurag Kumar}

In this lecture, we talk about the advantages of deep learning over "shallow approaches" and an overview of deep learning methods on graphs.
\section{What is Deep Learning}
Given data points $\{(X,y)\}$, using the techniques we have learnt until now(for e.g linear predictor) we cannot model non-linear functions. A workaround for this is the use of proper basis functions ($\phi(\vec{x})$) and kernel functions. The basis functions need not be linear in $x$ but the predictor is linear in the weights which allows it to easily model non-linear functions.
For eg, one such formulation can be (here the parameters to be trained are the $c_i$'s:-
\begin{align}
K(x,x') & = \Phi(x)^T\Phi(x') \\
y(x) & = \sum_{i\in X}c_iK(x,x_i)
\end{align}
Another method is neural networks. We can tell how powerful neural net can be by stating the fact that any Turing machine can be simulated by multilayer perceptron. Also, in a paper in 1999 Universal approximator was introduced which stated that if $f`$ is a non linear and continuous function,then we can approximate it as $f \circ h \circ g$ where  $f$ and $g$ are polynomial function and $h$ is a non-linear continuous function(e.g. RELU).
% TODO : 1992 paper \\
% TODO : Universal approximator
\section{Deep learning with respect to Graphs}
Thing learnt till now are 
\begin{enumerate}
    \item LP heuristics :- For supervised learning just take a weighted sum of scores ($w_{AA}.AA + w_{CN}.CN$) and learn for the weights
    \item Supervised random walk 
    \item Collaborative filtering
\end{enumerate}
\paragraph{What is the feature of each node?} : Node embeddings are nothing but compression of the graph. For each node we try to compress the $O(|V|)$ sized adjacency vector to a smaller size $d$ being the embedding dimension. So our goal will be to produce task agnostic node embedddings (independent of the task) which will be then converted to task specific node embeddings according to the task.

The node embeddings should be permutation invariant, i.e given the raw features $z_u$ and the embeddings $x_u$
\begin{align*}
    x_u & = f(z_1, z_2, ..., z_n) \\
    & = f(z_2, z_1, ..., z_n)
\end{align*}
or for any other permutation of $z_i$'s. For this to happen the function should be of the form $\Psi(\sum_i\Phi(z_i))$ as given in \cite{zaheer2018deep} where $\Psi$ is any real valued function and $\Phi$ are the basis functions (this is a necessary and sufficient condition for a function to be a set function).


Given any model to calculate the node embeddings, one would want the model parameters to be independent of $|V|$ and $|E|$.  The learning of the model can be done in two ways: -
\begin{enumerate}
    \item Transductive learning:- Given a new node, we will have to learn the whole model again so as to incorporate the new node (like node2vec where the node embeddings are like model parameters).
    \item Inductive learning:- We instead learn a function $f$ which models what the node embeddings should be ($x_u = f(N(u))$) and hence when given a new node we can directly output its embedding based on its neighbours (like GCN, graphSAGE, GNN).
\end{enumerate}
The main applications of node embeddings are Node Classification, Link Prediction and Graph Classification.
\bibliographystyle{acm}
\bibliography{ref}
\end{document}
